{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "\n",
    "from IPython import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 7, 7, 128)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    assert model.output_shape == (None, 14, 14, 64)\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 28, 28, 1)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd6d9e41dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00108524]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a single image using the epoch number\n",
    "def display_image(epoch_no):\n",
    "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "with imageio.get_writer(anim_file, mode='I') as writer:\n",
    "  filenames = glob.glob('image*.png')\n",
    "  filenames = sorted(filenames)\n",
    "  for filename in filenames:\n",
    "    image = imageio.imread(filename)\n",
    "    writer.append_data(image)\n",
    "  image = imageio.imread(filename)\n",
    "  writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
